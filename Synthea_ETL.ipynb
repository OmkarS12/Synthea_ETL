{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdd12a04-ddd3-4195-ad9d-929552e92ef3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67de90e1-dce9-48c1-b56b-cef2faf71508",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text('root_path','FileStore/')\n",
    "dbutils.widgets.text('synth_in', 'FileStore/shared_uploads/omkars1202@gmail.com/')\n",
    "root_path=dbutils.widgets.get('root_path')\n",
    "synth_in=dbutils.widgets.get('synth_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a21103-fba8-40aa-a87d-fa808b438412",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff26d99-e78a-4637-aa01-3722c2c8ee6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthea Raw Path: FileStore/shared_uploads/omkars1202@gmail.com/\n Delta Output Path:FileStore/delta/\n"
     ]
    }
   ],
   "source": [
    "synthea_path  = synth_in\n",
    "delta_root_path = f\"{root_path}delta/\"\n",
    "print(f'Synthea Raw Path: {synthea_path}\\n Delta Output Path:{delta_root_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c742724-d46f-4ba0-a828-de93a9d8afba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthea Raw Path: FileStore/shared_uploads/omkars1202@gmail.com/\nDelta Output Path: FileStore/delta/\n"
     ]
    }
   ],
   "source": [
    "print(f'Synthea Raw Path: {synthea_path}\\nDelta Output Path: {delta_root_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8012d770-a5d0-4d09-b94c-510695bc6876",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/allergies.csv</td><td>allergies.csv</td><td>70010</td><td>1708397768000</td></tr><tr><td>dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/careplans.csv</td><td>careplans.csv</td><td>672599</td><td>1708397768000</td></tr><tr><td>dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/conditions.csv</td><td>conditions.csv</td><td>1062972</td><td>1708397768000</td></tr><tr><td>dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/devices.csv</td><td>devices.csv</td><td>17498</td><td>1708397768000</td></tr><tr><td>dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/encounters.csv</td><td>encounters.csv</td><td>16474491</td><td>1708397774000</td></tr><tr><td>dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/imaging_studies.csv</td><td>imaging_studies.csv</td><td>205364</td><td>1708397769000</td></tr><tr><td>dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/immunizations.csv</td><td>immunizations.csv</td><td>2225013</td><td>1708397771000</td></tr><tr><td>dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/medications.csv</td><td>medications.csv</td><td>10884009</td><td>1708397775000</td></tr><tr><td>dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/observations.csv</td><td>observations.csv</td><td>43094924</td><td>1708397785000</td></tr><tr><td>dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/organizations.csv</td><td>organizations.csv</td><td>169568</td><td>1708397776000</td></tr><tr><td>dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/patients.csv</td><td>patients.csv</td><td>332570</td><td>1708397776000</td></tr><tr><td>dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/payer_transitions.csv</td><td>payer_transitions.csv</td><td>341479</td><td>1708397777000</td></tr><tr><td>dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/payers.csv</td><td>payers.csv</td><td>2200</td><td>1708397777000</td></tr><tr><td>dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/procedures.csv</td><td>procedures.csv</td><td>5695028</td><td>1708397780000</td></tr><tr><td>dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/providers.csv</td><td>providers.csv</td><td>1045666</td><td>1708397781000</td></tr><tr><td>dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/supplies.csv</td><td>supplies.csv</td><td>49</td><td>1708397782000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/allergies.csv",
         "allergies.csv",
         70010,
         1708397768000
        ],
        [
         "dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/careplans.csv",
         "careplans.csv",
         672599,
         1708397768000
        ],
        [
         "dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/conditions.csv",
         "conditions.csv",
         1062972,
         1708397768000
        ],
        [
         "dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/devices.csv",
         "devices.csv",
         17498,
         1708397768000
        ],
        [
         "dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/encounters.csv",
         "encounters.csv",
         16474491,
         1708397774000
        ],
        [
         "dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/imaging_studies.csv",
         "imaging_studies.csv",
         205364,
         1708397769000
        ],
        [
         "dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/immunizations.csv",
         "immunizations.csv",
         2225013,
         1708397771000
        ],
        [
         "dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/medications.csv",
         "medications.csv",
         10884009,
         1708397775000
        ],
        [
         "dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/observations.csv",
         "observations.csv",
         43094924,
         1708397785000
        ],
        [
         "dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/organizations.csv",
         "organizations.csv",
         169568,
         1708397776000
        ],
        [
         "dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/patients.csv",
         "patients.csv",
         332570,
         1708397776000
        ],
        [
         "dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/payer_transitions.csv",
         "payer_transitions.csv",
         341479,
         1708397777000
        ],
        [
         "dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/payers.csv",
         "payers.csv",
         2200,
         1708397777000
        ],
        [
         "dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/procedures.csv",
         "procedures.csv",
         5695028,
         1708397780000
        ],
        [
         "dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/providers.csv",
         "providers.csv",
         1045666,
         1708397781000
        ],
        [
         "dbfs:/FileStore/shared_uploads/omkars1202@gmail.com/supplies.csv",
         "supplies.csv",
         49,
         1708397782000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(synthea_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf1ec3bc-c2c1-43df-8654-da4333927af1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datasets= ['allergies',\n",
    "          'careplans',\n",
    "          'conditions',\n",
    "          'devices',\n",
    "          'encounters',\n",
    "          'imaging_studies',\n",
    "          'immunizations',\n",
    "          'medications',\n",
    "          'observations',\n",
    "          'organizations',\n",
    "          'patients',\n",
    "          'payer_transitions',\n",
    "          'payers',\n",
    "          'procedures',\n",
    "          'providers',\n",
    "          'supplies'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d378f6d9-8821-4735-80e6-990eb398eac6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. CSV Files as Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f77d39ab-5e4e-49a0-b1ef-ef099af4399d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a python dictionary of dataframes\n",
    "df_dict = {}\n",
    "for dataset in datasets:\n",
    "    df_dict[dataset] = spark.read.csv('dbfs:/{}/{}.csv'.format(synthea_path,dataset),header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ef3a68-e666-4951-897b-ce27ab051341",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame keys (file names): dict_keys(['allergies', 'careplans', 'conditions', 'devices', 'encounters', 'imaging_studies', 'immunizations', 'medications', 'observations', 'organizations', 'patients', 'payer_transitions', 'payers', 'procedures', 'providers', 'supplies'])\n"
     ]
    }
   ],
   "source": [
    "# Print the dictionary keys (file names)\n",
    "print(\"DataFrame keys (file names):\", df_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8788b952-733f-4059-a73a-e48f33d0c565",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dataset</th><th>n_records</th></tr></thead><tbody><tr><td>observations</td><td>299697</td></tr><tr><td>encounters</td><td>53346</td></tr><tr><td>medications</td><td>42989</td></tr><tr><td>procedures</td><td>34981</td></tr><tr><td>immunizations</td><td>15478</td></tr><tr><td>conditions</td><td>8376</td></tr><tr><td>providers</td><td>5855</td></tr><tr><td>payer_transitions</td><td>3801</td></tr><tr><td>careplans</td><td>3483</td></tr><tr><td>patients</td><td>1171</td></tr><tr><td>organizations</td><td>1119</td></tr><tr><td>imaging_studies</td><td>855</td></tr><tr><td>allergies</td><td>597</td></tr><tr><td>devices</td><td>78</td></tr><tr><td>payers</td><td>10</td></tr><tr><td>supplies</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "observations",
         299697
        ],
        [
         "encounters",
         53346
        ],
        [
         "medications",
         42989
        ],
        [
         "procedures",
         34981
        ],
        [
         "immunizations",
         15478
        ],
        [
         "conditions",
         8376
        ],
        [
         "providers",
         5855
        ],
        [
         "payer_transitions",
         3801
        ],
        [
         "careplans",
         3483
        ],
        [
         "patients",
         1171
        ],
        [
         "organizations",
         1119
        ],
        [
         "imaging_studies",
         855
        ],
        [
         "allergies",
         597
        ],
        [
         "devices",
         78
        ],
        [
         "payers",
         10
        ],
        [
         "supplies",
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "dataset",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "n_records",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataframes=[(x[0],x[1].count()) for x in list(df_dict.items())]\n",
    "display(pd.DataFrame(dataframes,columns=['dataset','n_records']).sort_values(by=['n_records'],ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f7ceb9-9e28-456b-98b7-54d522a8f80a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. De-identify Patient PHI (Masking to be performed in the Production ETL process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47743f93-115c-4109-92b8-c30101b4c617",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da49d01e-23b8-4c12-b87d-b8a8dd423902",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mask_pii(pii_col: pd.Series) -> pd.Series:\n",
    "    '''\n",
    "    mask_pii: function takes a pandas series and returned sha1 hash values of elements\n",
    "    '''\n",
    "    sha_value = pii_col.map(lambda x: hashlib.sha1(x.encode()).hexdigest())\n",
    "    return sha_value\n",
    " \n",
    "mask_pii_udf = pandas_udf(mask_pii, returnType=StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "580e0a5c-fd55-4899-85af-fca2765d0a3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We then use this function to mask pii columns for a given set of columns, namely:\n",
    "\n",
    "['SSN','DRIVERS','PASSPORT','PREFIX','FIRST','LAST','SUFFIX','MAIDEN','BIRTHPLACE','ADDRESS']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d99fad08-e8ae-42bc-b8d8-89c316350183",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pii_cols=['SSN','DRIVERS','PASSPORT','PREFIX','FIRST','LAST','SUFFIX','MAIDEN','BIRTHPLACE','ADDRESS']\n",
    "patients_obfuscated = df_dict['patients']\n",
    "for c in pii_cols:\n",
    "  masked_col_name = c+'_masked'\n",
    "  patients_obfuscated = patients_obfuscated.withColumn(c,F.coalesce(c,F.lit('null'))).withColumn(masked_col_name,mask_pii_udf(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61019acc-3f75-449e-84fa-471ea1a5c871",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Replacing the Original records with the de-identified records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85583436-31d2-4942-95aa-834c53cd9d2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dict['patients']=patients_obfuscated.drop(*pii_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fff5f83-c029-474d-94c5-8b13920598dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4. Writing Tables to Dellta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8a545d5-3e12-45cd-bb88-ffbd6719aa01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta tables will be stored in FileStore/delta/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  dbutils.fs.ls(delta_root_path)\n",
    "except:\n",
    "  print(f'Path {delta_root_path} does not exist, creating path {delta_root_path}')\n",
    "  dbutils.fs.mkdirs(delta_root_path)\n",
    "print(f'Delta tables will be stored in {delta_root_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f372066d-a6e9-4474-b02a-2cfb05e2a9d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table_name in datasets:\n",
    "  table_path = f'dbfs:/FileStore/delta'+ '/bronze/{}'.format(table_name)\n",
    "  df_dict[table_name].write.format('delta').mode(\"overwrite\").save(table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77a89ef9-3967-4383-b725-243d9c6b08f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/delta/bronze/allergies/</td><td>allergies/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/delta/bronze/careplans/</td><td>careplans/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/delta/bronze/conditions/</td><td>conditions/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/delta/bronze/devices/</td><td>devices/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/delta/bronze/encounters/</td><td>encounters/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/delta/bronze/imaging_studies/</td><td>imaging_studies/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/delta/bronze/immunizations/</td><td>immunizations/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/delta/bronze/medications/</td><td>medications/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/delta/bronze/observations/</td><td>observations/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/delta/bronze/organizations/</td><td>organizations/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/delta/bronze/patients/</td><td>patients/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/delta/bronze/payer_transitions/</td><td>payer_transitions/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/delta/bronze/payers/</td><td>payers/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/delta/bronze/procedures/</td><td>procedures/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/delta/bronze/providers/</td><td>providers/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/delta/bronze/supplies/</td><td>supplies/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/delta/bronze/allergies/",
         "allergies/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/delta/bronze/careplans/",
         "careplans/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/delta/bronze/conditions/",
         "conditions/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/delta/bronze/devices/",
         "devices/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/delta/bronze/encounters/",
         "encounters/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/delta/bronze/imaging_studies/",
         "imaging_studies/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/delta/bronze/immunizations/",
         "immunizations/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/delta/bronze/medications/",
         "medications/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/delta/bronze/observations/",
         "observations/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/delta/bronze/organizations/",
         "organizations/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/delta/bronze/patients/",
         "patients/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/delta/bronze/payer_transitions/",
         "payer_transitions/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/delta/bronze/payers/",
         "payers/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/delta/bronze/procedures/",
         "procedures/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/delta/bronze/providers/",
         "providers/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/delta/bronze/supplies/",
         "supplies/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(f'{delta_root_path}/bronze/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28eca5cc-0562-4fbc-bdca-e591da9db999",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[148]: True"
     ]
    }
   ],
   "source": [
    "silver_path = \"dbfs:/FileStore/delta/Silver/\"\n",
    "\n",
    "# the Silver layer directory\n",
    "dbutils.fs.mkdirs(silver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7e0567f-d9d9-4b32-99e9-3a0a39eb3382",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-817673786071447>:17\u001B[0m\n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Process each dataset\u001B[39;00m\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n",
       "\u001B[0;32m---> 17\u001B[0m     clean_and_transform_df(dataset)\n",
       "\n",
       "File \u001B[0;32m<command-817673786071447>:13\u001B[0m, in \u001B[0;36mclean_and_transform_df\u001B[0;34m(dataset)\u001B[0m\n",
       "\u001B[1;32m     10\u001B[0m df\u001B[38;5;241m.\u001B[39mdropna()\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Write cleaned and transformed data to silver layer\u001B[39;00m\n",
       "\u001B[0;32m---> 13\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdelta\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43msilver_path\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdataset\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1397\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1395\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n",
       "\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1397\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Cannot write to already existent path dbfs:/FileStore/delta/Silver/allergies without setting OVERWRITE = 'true'."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-817673786071447>:17\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Process each dataset\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[0;32m---> 17\u001B[0m     clean_and_transform_df(dataset)\n\nFile \u001B[0;32m<command-817673786071447>:13\u001B[0m, in \u001B[0;36mclean_and_transform_df\u001B[0;34m(dataset)\u001B[0m\n\u001B[1;32m     10\u001B[0m df\u001B[38;5;241m.\u001B[39mdropna()\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Write cleaned and transformed data to silver layer\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdelta\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43msilver_path\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdataset\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1397\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1395\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1397\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Cannot write to already existent path dbfs:/FileStore/delta/Silver/allergies without setting OVERWRITE = 'true'.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Cannot write to already existent path dbfs:/FileStore/delta/Silver/allergies without setting OVERWRITE = 'true'.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define path for Bronze and Silver layers\n",
    "bronze_path = \"dbfs:/FileStore/delta/bronze/\"\n",
    "silver_path = \"dbfs:/FileStore/delta/Silver/\"\n",
    "\n",
    "# Function to read Delta table and return cleaned and transformed DataFrame\n",
    "def clean_and_transform_df(dataset):\n",
    "    df = spark.read.format(\"delta\").load(f\"{bronze_path}{dataset}\")\n",
    "    # Here are some examples, adjust based on your needs:\n",
    "    df.dropDuplicates()\n",
    "    df.dropna()\n",
    "\n",
    "    # Write cleaned and transformed data to silver layer\n",
    "    df.write.format(\"delta\").save(f\"{silver_path}{dataset}\")\n",
    "\n",
    "# Process each dataset\n",
    "for dataset in datasets:\n",
    "    clean_and_transform_df(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "123903d6-144d-4302-aedd-c026a38ac2e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_df = clean_and_transform_df('patients')\n",
    "clean_df.write.quality_checked_df.write.mode(\"overwrite\").format(\"delta\").save(silver_path + dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1003fe49-e308-472c-bdc4-ee7e6d7c4d05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Synthea_ETL",
   "widgets": {
    "root_path": {
     "currentValue": "FileStore/",
     "nuid": "b9e5380b-d272-46d9-8677-b1e34a90679e",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "FileStore/",
      "label": null,
      "name": "root_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "synth_in": {
     "currentValue": "FileStore/shared_uploads/omkars1202@gmail.com/",
     "nuid": "be96e588-3cd0-4848-8603-c302aa83331f",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "FileStore/shared_uploads/omkars1202@gmail.com/",
      "label": null,
      "name": "synth_in",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
